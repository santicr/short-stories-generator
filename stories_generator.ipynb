{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Stories generator first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temas = [\"aventura\", \"misterio\", \"ciencia ficción\", \"romance\", \"fantasía\"]\n",
    "generos = [\"comedia\", \"tragedia\", \"acción\", \"drama\", \"suspense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_historia(tema, genero):\n",
    "    # Define una lista de posibles introducciones.\n",
    "    introducciones = [\"En un lejano reino\", \"En la ciudad de\", \"En un futuro distópico\", \"Había una vez\"]\n",
    "\n",
    "    # Selecciona una introducción aleatoria.\n",
    "    introduccion = random.choice(introducciones)\n",
    "\n",
    "    # Construye la historia base.\n",
    "    historia_base = f\"{introduccion}, {tema} {genero}.\"\n",
    "    \n",
    "    # Agrega un giro inesperado.\n",
    "    giros = [\"Pero lo que nadie sabía era que\", \"Sin embargo, todo cambió cuando\", \"De repente, un extraño evento ocurrió:\"]\n",
    "\n",
    "    historia_con_giro = f\"{historia_base} {random.choice(giros)} {tema} se enfrentó a un inesperado desafío.\"\n",
    "\n",
    "    return historia_con_giro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historia Generada:\n",
      "\n",
      "En un lejano reino, misterio drama. Pero lo que nadie sabía era que misterio se enfrentó a un inesperado desafío.\n",
      "Historia guardada en 'historia_generada.txt'.\n"
     ]
    }
   ],
   "source": [
    "tema_elegido = input(\"Por favor, elige un tema: \").lower()\n",
    "genero_elegido = input(\"Por favor, elige un género: \").lower()\n",
    "\n",
    "\n",
    "if tema_elegido not in temas or genero_elegido not in generos:\n",
    "    print(\"Tema o género no válido. Por favor, elige uno de los temas y géneros disponibles.\")\n",
    "else:\n",
    "    # Generar la historia.\n",
    "    historia_generada = generar_historia(tema_elegido, genero_elegido)\n",
    "\n",
    "    # Mostrar la historia.\n",
    "    print(\"\\nHistoria Generada:\\n\")\n",
    "    print(historia_generada)\n",
    "\n",
    "    # Opcionalmente, guardar o imprimir la historia en un archivo.\n",
    "    opcion = input(\"¿Deseas guardar o imprimir la historia? (s/n): \").lower()\n",
    "    if opcion == \"s\":\n",
    "        with open(\"historia_generada.txt\", \"w\") as archivo:\n",
    "            archivo.write(historia_generada)\n",
    "            print(\"Historia guardada en 'historia_generada.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo pre-entrenado GPT-2 y el tokenizador\n",
    "modelo = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizador = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Crear una historia base\n",
    "historia_base = \"En un lejano reino, un valiente caballero se aventuró\"\n",
    "\n",
    "# Generar texto con una longitud máxima más corta\n",
    "input_ids = tokenizador.encode(historia_base, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones(input_ids.shape)\n",
    "outputs = modelo.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, attention_mask=attention_mask)\n",
    "\n",
    "historia_generada = tokenizador.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lejano reino, un valiente caballero se aventuró a la vida de la cosa de los cualquieros.\n",
      "\n",
      "\"I am not a man of the world, but I am a woman of a different world. I have been born in a world of men, and I know that I will be a part of it. But I do not know what I want to do with my life. It is not my business to\n"
     ]
    }
   ],
   "source": [
    "print(historia_generada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short stories generator second approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: Short story genre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi user!\n",
      "Welcome to the short story generator, please type your favorite genre!\n",
      "The genres are the following: \n",
      "romance\n",
      "adventure\n",
      "mystery-&-detective\n",
      "fantasy\n",
      "humor-&-comedy\n",
      "paranormal\n",
      "science-fiction\n"
     ]
    }
   ],
   "source": [
    "genres = [\"romance\", \"adventure\", \"mystery-&-detective\", \"fantasy\", \"humor-&-comedy\", \"paranormal\", \"science-fiction\"]\n",
    "\n",
    "print(\"Hi user!\")\n",
    "print(\"Welcome to the short story generator, please type your favorite genre!\")\n",
    "print(\"The genres are the following: \")\n",
    "\n",
    "for genre in genres:\n",
    "    print(genre)\n",
    "\n",
    "genre = input(\"Type the genre here: \")\n",
    "\n",
    "while genre not in genres:\n",
    "    print(\"Please type a genre that is on the list\")\n",
    "    genre = input(\"Type the genre here: \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_name = dotenv_values(\".env\")[\"NAME\"]  # config = {\"USER\": \"foo\", \"EMAIL\": \"foo@example.org\"}\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "input_prompt = f\"<BOS> <{genre}>\"\n",
    "story = generator(input_prompt, max_length=400, do_sample=True,repetition_penalty=1.5, temperature=1.2, \n",
    "               top_p=0.95, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<BOS> <fantasy> ### ## Chapter 7, About The Book of Revelation #4 \"Not all I could bring back would be lost.\" Inability to find out if the word was true. How much trouble he had in finding himself on a trail... If his answers were yes it wasn\\'t that bad for him....but how does this help? Would God really do something like these and take off into nowhere after being caught by some unscrupulous scold?! It certainly doesn! He just need somebody as good at catching criminals. But if there is any reason behind bringing Mr Prager onto our feet we should give up having been held prisoner there while John went ahead with us without further problem but then Paul took away what opportunity and money he gave her anyway instead saying nothing more but an empty shrug as he ate another fruit bag filled breakfast today  And did not let anybody hear his tale until they thought their life didn`t make sense right now - since now he only wanted three days stay; though no wonder Jesus called out \\'we hope you\\'ll come through.\\' The two other things left unsaid were when we got into cars they made short circles around one car before following where the man led them as well getting rid – probably taking turns driving fast passing every ten minutes, whilst going on an errand with some extra stuff about his clothes worn down to get rid from its damage which he never needed again. He kept it clean as many times as possible with none asking once to pass through for permission. This week came along fineing soothed or whatever it meant  As he went back over here I knew he didnned several places with it, and did wash himself twice again once the first time which was great because his clothes are always ready too. Now knowing why the previous day\\'s receptionists arrived in town as though it might keep people amused he said he\\'d want someone else running water before the others. There wasn enough room in my dress-suit closet upstairs. My dad asked me'}]\n"
     ]
    }
   ],
   "source": [
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
