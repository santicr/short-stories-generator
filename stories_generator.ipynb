{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Stories generator first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temas = [\"aventura\", \"misterio\", \"ciencia ficción\", \"romance\", \"fantasía\"]\n",
    "generos = [\"comedia\", \"tragedia\", \"acción\", \"drama\", \"suspense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_historia(tema, genero):\n",
    "    # Define una lista de posibles introducciones.\n",
    "    introducciones = [\"En un lejano reino\", \"En la ciudad de\", \"En un futuro distópico\", \"Había una vez\"]\n",
    "\n",
    "    # Selecciona una introducción aleatoria.\n",
    "    introduccion = random.choice(introducciones)\n",
    "\n",
    "    # Construye la historia base.\n",
    "    historia_base = f\"{introduccion}, {tema} {genero}.\"\n",
    "    \n",
    "    # Agrega un giro inesperado.\n",
    "    giros = [\"Pero lo que nadie sabía era que\", \"Sin embargo, todo cambió cuando\", \"De repente, un extraño evento ocurrió:\"]\n",
    "\n",
    "    historia_con_giro = f\"{historia_base} {random.choice(giros)} {tema} se enfrentó a un inesperado desafío.\"\n",
    "\n",
    "    return historia_con_giro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historia Generada:\n",
      "\n",
      "En un lejano reino, misterio drama. Pero lo que nadie sabía era que misterio se enfrentó a un inesperado desafío.\n",
      "Historia guardada en 'historia_generada.txt'.\n"
     ]
    }
   ],
   "source": [
    "tema_elegido = input(\"Por favor, elige un tema: \").lower()\n",
    "genero_elegido = input(\"Por favor, elige un género: \").lower()\n",
    "\n",
    "\n",
    "if tema_elegido not in temas or genero_elegido not in generos:\n",
    "    print(\"Tema o género no válido. Por favor, elige uno de los temas y géneros disponibles.\")\n",
    "else:\n",
    "    # Generar la historia.\n",
    "    historia_generada = generar_historia(tema_elegido, genero_elegido)\n",
    "\n",
    "    # Mostrar la historia.\n",
    "    print(\"\\nHistoria Generada:\\n\")\n",
    "    print(historia_generada)\n",
    "\n",
    "    # Opcionalmente, guardar o imprimir la historia en un archivo.\n",
    "    opcion = input(\"¿Deseas guardar o imprimir la historia? (s/n): \").lower()\n",
    "    if opcion == \"s\":\n",
    "        with open(\"historia_generada.txt\", \"w\") as archivo:\n",
    "            archivo.write(historia_generada)\n",
    "            print(\"Historia guardada en 'historia_generada.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo pre-entrenado GPT-2 y el tokenizador\n",
    "modelo = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizador = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Crear una historia base\n",
    "historia_base = \"En un lejano reino, un valiente caballero se aventuró\"\n",
    "\n",
    "# Generar texto con una longitud máxima más corta\n",
    "input_ids = tokenizador.encode(historia_base, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones(input_ids.shape)\n",
    "outputs = modelo.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, attention_mask=attention_mask)\n",
    "\n",
    "historia_generada = tokenizador.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lejano reino, un valiente caballero se aventuró a la vida de la cosa de los cualquieros.\n",
      "\n",
      "\"I am not a man of the world, but I am a woman of a different world. I have been born in a world of men, and I know that I will be a part of it. But I do not know what I want to do with my life. It is not my business to\n"
     ]
    }
   ],
   "source": [
    "print(historia_generada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short stories generator second approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: Short story genre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi user!\n",
      "Welcome to the short story generator, please type your favorite genre!\n",
      "The genres are the following: \n",
      "romance\n",
      "adventure\n",
      "mystery-&-detective\n",
      "fantasy\n",
      "humor-&-comedy\n",
      "paranormal\n",
      "science-fiction\n"
     ]
    }
   ],
   "source": [
    "genres = [\"romance\", \"adventure\", \"mystery-&-detective\", \"fantasy\", \"humor-&-comedy\", \"paranormal\", \"science-fiction\"]\n",
    "\n",
    "print(\"Hi user!\")\n",
    "print(\"Welcome to the short story generator, please type your favorite genre!\")\n",
    "print(\"The genres are the following: \")\n",
    "\n",
    "for genre in genres:\n",
    "    print(genre)\n",
    "\n",
    "genre = input(\"Type the genre here: \")\n",
    "\n",
    "while genre not in genres:\n",
    "    print(\"Please type a genre that is on the list\")\n",
    "    genre = input(\"Type the genre here: \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "input_prompt = f\"<BOS> <{genre}>\"\n",
    "story = generator(input_prompt, max_length=400, do_sample=True,repetition_penalty=1.5, temperature=1.2, \n",
    "               top_p=0.95, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<BOS> <adventure> I\\'d never been happier to meet with the man that led them around! Thank you. The way his lips worked was so smooth, as if he had just finished something amazing about being an actor who could handle my life by acting for me - he always thought it would take a long time before our relationship really fell apart... Then there is another piece of information that must be kept secret in order not ever again tarnished and put upon some unsuspecting audience member, even when she wasn\\'t looking or smiling at all.... Well anyway then on reflection this whole divorce happened because we\\'d both felt like one side needed him out.\" He laughed softly into her ear, \"I understand why Mr Merman thinks these words are wonderful, but this does bother us since sometimes we\\'ve got too muchtogether during breakups to make good news though!\"  She shook my hand gently now knowing what each other were having already said. And now they didn\\'t have anything else going back until she did: That conversation took place several hours ahead notice...\" Now, a statement here. She sat up, holding tight hands beside Ben\\'s; but as if, she closed their eyes momentarily then spoke quietly, \"Letters from Tarno to yourself,\" while simultaneously saying a little more carefully than I could give if that weren\\' necessary\". When she came closer Kateria continued standing on two chairs. His hands clasped behind Katety Latham so tightly in anticipation where will his sister go?  They turned away leaving Katey at first  A lot has followed between them... After waiting until their departure party went over the door... Next stop everyone\\'s talk   Once at the top level stairs into town Keats stood across from the exit leading through everything except himself inside. They gave the men entrance onto four different floors and two doors to which was another corridor which looked absolutely perfect. Inside she passed on his cell phone. In front walked her old-age hair blonde blue jeans,'}]\n"
     ]
    }
   ],
   "source": [
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
